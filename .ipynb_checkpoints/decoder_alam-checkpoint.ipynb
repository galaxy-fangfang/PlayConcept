{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5358301-95b4-4d41-9614-6280a839f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoderutils.utils import base_prompt, base_prompt_with_expression, expression_prompt, get_words, partition_candidates, encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f9683d-019a-45be-9691-c25fab92680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "ollama = Ollama(base_url='http://localhost:11434', model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78880d1-0af2-4ba8-911a-85ef1a2773bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llama(prompt):\n",
    "    answer = ollama(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3222dce-57c0-4ce3-b204-4b712e9c8ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(code, candidates):\n",
    "    \"\"\"Decoder for Concept.\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    code: list of lists of integers\n",
    "        Each list corresponds to a marker, starting with the green.\n",
    "        The first element of each list is the marker (= question / exclamation mark), the others are the attributes (= cubes).\n",
    "        There are at most 5 lists, each of length at most 10.\n",
    "\n",
    "    candidates: list of str\n",
    "        Candidate concepts.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    concept: str\n",
    "        Concept.\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    Candidates = ['Apple', 'Honey', 'House']\n",
    "    Code = [[26], [6]]\n",
    "    Decoding = Food (main concept) related to an animal (secondary concept).\n",
    "    Expected concept = 'Honey'\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Samuel: Here is a suggestion that uses template prompts from the decoderutils/utils.py. Th eidea it to use each color to filter the list and to return the final list after all filters. To help the model, I divide the list into chunks (the size can be set in the function`partition_candidates`). Finally, to account for complex strategy such a discribing a country through its flag, I also add a prompt to ask the model to suggest an expression based on the code.\n",
    "    \n",
    "    # Retrieve the content from codes.csv file\n",
    "    codes=[]\n",
    "\n",
    "    with open(\"codes.csv\") as file:\n",
    "        lines = file.read().split(\"\\n\")\n",
    "        codes = [line.split(\",\") for line in lines]\n",
    "        #print(codes)\n",
    "    \n",
    "    # Translate the codes into the corresponding words\n",
    "    nl_codes = get_words(code, codes)\n",
    "    #print(nl_codes)\n",
    "    \n",
    "    # For each concept (each color), ask OLLAMA to filter the current list of candidates based either on the codes or on the expression\n",
    "    for sub_code in nl_codes:\n",
    "\n",
    "        #print(sub_code)\n",
    "    \n",
    "        # Ask OLLAMA for an expression based on the prompt, e.g. a \"flag,country,location\" that is \"cross\" and \"red\" and \"white\" and \"blue\" is the \"union jack\" or england\n",
    "        prompt = expression_prompt.format(f'\"{sub_code[0].replace(\",\",\", \")}\"', \" and \".join([f'\"{elt}\"' for elt in sub_code[1:]]))\n",
    "        print(prompt)\n",
    "        expression = ask_llama(prompt)\n",
    "        print(expression)\n",
    "    \n",
    "        ## Get answers based on the code and on the expression\n",
    "        '''filter_candidates = set()\n",
    "    \n",
    "        # Divide the list of candidates to hep the model\n",
    "        for subset_candidates in partition_candidates(candidates, chunk_size=50):\n",
    "            # Ask OLLAMA for the answer based on the code\n",
    "            prompt = base_prompt.format(f'\"{sub_code[0]}\"', \" and \".join([f'\"{elt}\"' for elt in sub_code[1:]]),\"\\n\".join(subset_candidates))\n",
    "            filter_candidates |= set(ask_OLLAMA(prompt))\n",
    "        \n",
    "            # Ask OLLAMA for the answer based on the expression\n",
    "            prompt = base_prompt_with_expression.format(f'\"{expression}\"', \"\\n\".join(subset_candidates))\n",
    "            filter_candidates |= set(ask_OLLAMA(prompt))\n",
    "            \n",
    "            # Consider the selected elements as the candidate for next concept\n",
    "            candidates = list(filter_candidates)'''\n",
    "    \n",
    "    # Return the set of filtered candidates after each concept because it seems that the instructions have changed about the output type (a list instead of a single gess)\n",
    "    return candidates\n",
    "    \n",
    "    # concept = ''\n",
    "    # return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
